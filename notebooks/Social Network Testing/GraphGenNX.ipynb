{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import networkx as nx\n",
    "import os\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES\n",
    "DATA = \"/ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/\"\n",
    "OUT = '/ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/graphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING CONDENSED AUTHOR-SUBREDDIT DICTIONARY (COMBINE ALL 12 MONTHS INTO ONE DICTIONARY)\n",
    "CONDENSED = {}\n",
    "\n",
    "# READ IN MONTH BY MONTH\n",
    "def condense(filename):\n",
    "    # Setting path to file to read in\n",
    "    file_path = os.path.join(DATA, filename)\n",
    "\n",
    "    print(f\"Reading {file_path} ...\")\n",
    "\n",
    "    # Open the data\n",
    "    DICT = {}\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        DICT = json.load(json_file)\n",
    "    \n",
    "    # Iterate over all authors\n",
    "    for author in list(DICT.keys()):\n",
    "\n",
    "        # Key check\n",
    "        if author not in CONDENSED:\n",
    "            CONDENSED[author] = {}\n",
    "\n",
    "        # Add up subreddit interactions\n",
    "        for subreddit in DICT[author]:\n",
    "\n",
    "            # Key check\n",
    "            if subreddit not in CONDENSED[author]:\n",
    "                CONDENSED[author][subreddit] = 0\n",
    "            \n",
    "            # Incrementing    \n",
    "            CONDENSED[author][subreddit] += DICT[author][subreddit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOLERANCE: Only consider subreddits of an author where they have posted >= TOLERANCE times\n",
    "\n",
    "def generate_graph(TOLERANCE):\n",
    "    # Create an empty graph\n",
    "    g = nx.Graph()\n",
    "\n",
    "    # Adding edges and nodes\n",
    "    for author in CONDENSED:\n",
    "\n",
    "        # Keeping subreddits where posts >= TOLERANCE\n",
    "        keep = []\n",
    "        for subreddit in CONDENSED[author]:\n",
    "            if CONDENSED[author][subreddit] >= TOLERANCE:\n",
    "                keep.append(subreddit)\n",
    "\n",
    "        # If less than two subreddits left, cannot create an edge, no cross-community engagement\n",
    "        if len(keep) < 2:\n",
    "            continue\n",
    "\n",
    "        # Add vertices\n",
    "        for subreddit in keep:\n",
    "            # Add vertices, duplicates taken care of by NX\n",
    "            g.add_node(subreddit)\n",
    "\n",
    "        # Add edges\n",
    "        for subreddit1 in keep:\n",
    "            for subreddit2 in keep:\n",
    "                # No self cycles\n",
    "                if subreddit1 == subreddit2:\n",
    "                    continue\n",
    "                \n",
    "                # Adding 0.5 instead of 1 since each edge is considered twice in this nested loop\n",
    "                # That is, weights are added for A-->B and B-->A, hence 0.5 + 0.5 = 1 as we want\n",
    "                \n",
    "                # Check if the edge already exists\n",
    "                if g.has_edge(subreddit1, subreddit2):\n",
    "                    g[subreddit1][subreddit2]['weight'] += 0.5\n",
    "                # New edge\n",
    "                else:\n",
    "                    g.add_edge(subreddit1, subreddit2)\n",
    "                    g[subreddit1][subreddit2]['weight'] = 0.5\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-11.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-09.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-05.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-01.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-04.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-10.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-08.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-06.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-02.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-12.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-07.json ...\n",
      "Reading /ais/hal9000/datasets/reddit/stance_pipeline/luo_data/network_analysis/dictionaries/RC_2019-03.json ...\n"
     ]
    }
   ],
   "source": [
    "# Generate the CONDENSED dictionary\n",
    "# Considering all types of posts (Top and Non-Top)\n",
    "for filename in os.listdir(DATA):\n",
    "    if \"_top\" not in filename:\n",
    "        condense(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating size attribute for each community:\n",
    "POSTS = {}\n",
    "UNIQUE_USERS = {}\n",
    "\n",
    "for author in CONDENSED:\n",
    "    for subreddit in CONDENSED[author]:\n",
    "\n",
    "        # Key Check\n",
    "        if subreddit not in POSTS:\n",
    "            POSTS[subreddit] = 0\n",
    "        \n",
    "        if subreddit not in UNIQUE_USERS:\n",
    "            UNIQUE_USERS[subreddit] = 0\n",
    "\n",
    "        # Increment\n",
    "        POSTS[subreddit] += CONDENSED[author][subreddit]\n",
    "        UNIQUE_USERS[subreddit] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graphs\n",
    "# 100 ~ 10s\n",
    "# 20 ~ 15s\n",
    "# 10 ~ 30s\n",
    "# 5 ~ 60s\n",
    "TOLERANCE = 5\n",
    "g = generate_graph(TOLERANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Process\n",
    "# Set all subreddits loyal users to 0, preparing for next step in pipeline\n",
    "# Set the size attribute to the number of posts made in that subreddit\n",
    "\n",
    "for subreddit in g.nodes():    \n",
    "    # Loyalty\n",
    "    g.nodes[subreddit]['loyal_users'] = 0\n",
    "\n",
    "    # Posts\n",
    "    g.nodes[subreddit]['posts'] = POSTS[subreddit]\n",
    "\n",
    "    # Unique Users\n",
    "    g.nodes[subreddit]['unique_users'] = UNIQUE_USERS[subreddit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graph\n",
    "nx.write_graphml_lxml(g, OUT + f\"2019_NX_T{TOLERANCE}.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT NOTEBOOK ==> ComputeLoyalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
